{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link to download the Chrome webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://chromedriver.chromium.org/downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('/Users/anthonyvalle/Downloads/Chrome/chromedriver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This link is a webpage filled with other links that direct you to data in the form of html tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.nuforc.org/webreports/ndxpost.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  The variable 'links' is going to cointain all the 'a' tags on the wepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "driver.get(url)\n",
    "links = driver.find_elements_by_tag_name('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each element in the variable 'links', which is a list of all 'a' tag elements, we append the text of the actual tag to a list called link_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first 'a' tag within that webpage has a link that is not useful therfore it is popped off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "link_list = []\n",
    "for i in links:\n",
    "    link_list.append(i.text)\n",
    "link_list.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The function I am going to use to scrape all the data when each 'a' tag is clicked from the list of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraperows():\n",
    "    # Create an empty list\n",
    "    rows=[]\n",
    "    \n",
    "    # Gather all 'td' tags in the html table, essentially each table cell\n",
    "    body = driver.find_elements_by_tag_name('td')\n",
    "    \n",
    "    # A counter that will help in grouping a certain number of cells to represent a row\n",
    "    # This is important because we are scrapinf indivdual cells, not rows\n",
    "    counter = 0\n",
    "    \n",
    "    # A temporary list that will hold all the cells of one indvidual row\n",
    "    templist = []\n",
    "    \n",
    "    for elements in body:\n",
    "        \n",
    "        # Gather the text of the 'td' tag\n",
    "        text = elements.text\n",
    "        \n",
    "        # Append that text into a temporary list\n",
    "        templist.append(text)\n",
    "        \n",
    "        # Counter increment\n",
    "        counter += 1\n",
    "        \n",
    "        # If the counter equals 7 then we have scraped all the cells of an individual row and must seperate it\n",
    "        if counter == 7:\n",
    "            \n",
    "            #Reset the Counter\n",
    "            counter = 0\n",
    "            \n",
    "            # Append a copy of the temporary list to the list 'rows'\n",
    "            # We must append a copy becuase the list will be cleared which will clear it within the list 'rows'\n",
    "            rows.append(templist.copy())\n",
    "            \n",
    "            # Clearing the temporary list\n",
    "            templist.clear()\n",
    "    \n",
    "    # Procedure to seperate the time and date since they are in the same cell\n",
    "    # At this point in time the list 'rows' is filled with individual rows of all the cells we scraped\n",
    "    for num, elements in enumerate(rows):\n",
    "        \n",
    "        # Gather the element with the date and time which is always the first element\n",
    "        date_time = rows[num][0]\n",
    "        \n",
    "        # Split the date and time\n",
    "        split = date_time.split(' ')\n",
    "        \n",
    "        # Remove the concatenated date and time\n",
    "        rows[num].remove(date_time)\n",
    "        \n",
    "        # Insert the indivdual date and time into the row\n",
    "        for i in split:\n",
    "\n",
    "            rows[num].insert(0, i)\n",
    "            \n",
    "    return(rows)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = scraperows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The scraping procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list 'all_rows' will be the final list of ALL scraped rows\n",
    "all_rows = []\n",
    "\n",
    "# For each element in the list of the link texts\n",
    "for i in link_list:\n",
    "    \n",
    "    time.sleep(.5)\n",
    "    \n",
    "    # Find the link by link text\n",
    "    link = driver.find_element_by_link_text(i)\n",
    "    \n",
    "    # Click the link\n",
    "    link.click()\n",
    "    \n",
    "    # Scrape the html table and assign it to 'rows'\n",
    "    rows = scraperows()\n",
    "    \n",
    "    # Append every scraped row into the final list from line 1, 'all_rows'\n",
    "    for lists in rows:\n",
    "        \n",
    "        all_rows.append(lists)\n",
    "    \n",
    "    # Clear 'rows'\n",
    "    rows.clear()\n",
    "\n",
    "    time.sleep(.5)\n",
    "    \n",
    "    # Go back to the main webpage with all the links anmd repeat\n",
    "    driver.back()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After 2 hours of scraping the code mysteriously failed but I was able to scrape about 40,000 rows of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert 'all_rows' to a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop two odd columns that contained nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([8,9], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the DataFrame to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('UFO.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After the scraper failed I found the date of the link where it left off\n",
    "# Using this date I found the index of the link where the scraper failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in link_list:\n",
    "    if i == '05/29/2011':\n",
    "        print(link_list.index(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the index I created a new list of the links with only the links that werent scraped and continued to scrape my data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlist = link_list[370::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same process as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_all_rows = []\n",
    "\n",
    "for i in newlist:\n",
    "        \n",
    "    time.sleep(.5)\n",
    "    link = driver.find_element_by_link_text(i)\n",
    "    link.click()\n",
    "    \n",
    "    rows = scraperows()\n",
    "    \n",
    "    for lists in rows:\n",
    "        \n",
    "        new_all_rows.append(lists)\n",
    "      \n",
    "    rows.clear()\n",
    "\n",
    "    time.sleep(.5)\n",
    "\n",
    "    driver.back()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After 2 more hours of scraping I was able to get about 65,000 more rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata =pd.DataFrame(new_all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.drop([8,9,10,11,12], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.to_csv('NewUFO.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
